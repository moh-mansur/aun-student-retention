{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install owlready2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uORU2qCTnK3p",
        "outputId": "6944413e-668c-427b-9abc-47453d22d841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: owlready2 in /usr/local/lib/python3.12/dist-packages (0.50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "da169c60",
        "outputId": "d3d61272-ab28-4bc8-acb0-d63ddf960f0d"
      },
      "source": [
        "from flask import Flask, render_template_string, request, flash\n",
        "from owlready2 import get_ontology, sync_reasoner\n",
        "import joblib\n",
        "import numpy as np\n",
        "import threading\n",
        "from google.colab import files\n",
        "\n",
        "# Cell 3: Upload files\n",
        "print(\"Upload AUN_SRO.owl\")\n",
        "uploaded_owl = files.upload()\n",
        "owl_filename = list(uploaded_owl.keys())[0]\n",
        "\n",
        "print(\"\\nUpload best_rf_model.pkl\")\n",
        "uploaded_model = files.upload()\n",
        "model_filename = list(uploaded_model.keys())[0]\n",
        "\n",
        "# Cell 4: Load files\n",
        "onto = get_ontology(f\"file://{owl_filename}\").load()\n",
        "print(\"Ontology loaded!\")\n",
        "\n",
        "best_model = joblib.load(model_filename)\n",
        "print(\"ML model loaded!\")\n",
        "\n",
        "# Cell 5: Flask app (your format, updated)\n",
        "app = Flask(__name__)\n",
        "app.debug = True # Enable debug mode to see detailed errors\n",
        "\n",
        "# Your 3 students (hardcoded IDs, but risk/recommendations pulled from ontology)\n",
        "student_ids = ['A00080012', 'A00080013', 'A00080014']\n",
        "\n",
        "# Define features_order for ML prediction from ontology data\n",
        "features_order = ['gpa', 'balance', 'logins', 'missed', 'incidents', 'sessionsAttended']\n",
        "\n",
        "# Re-introducing ONTOLOGY_RECOMMENDATIONS based on user feedback\n",
        "ONTOLOGY_RECOMMENDATIONS = {\n",
        "    'High': 'Immediate financial aid consultation<br>Mandatory advising session required',\n",
        "    'Medium': 'Academic monitoring and possible advising',\n",
        "    'Low': 'No recommendation inferred, student doing well'\n",
        "}\n",
        "\n",
        "TEMPLATE = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>AUN Student Attrition Decision Support</title>\n",
        "    <style>\n",
        "        body {font-family: Arial, sans-serif; margin: 0; background: linear-gradient(to right, #ece9e6, #ffffff); color: #333; transition: background-color 0.3s, color 0.3s;}\n",
        "        .dark-mode body { background: linear-gradient(to right, #2c3e50, #1a252f); color: #ecf0f1; }\n",
        "        h1, h2 {color: #2c3e50; text-align: center; margin-bottom: 20px; transition: color 0.3s;}\n",
        "        .dark-mode h1, .dark-mode h2 { color: #ecf0f1; }\n",
        "        .header-container {text-align: center; padding: 20px; background-color: #3498db; color: white; margin-bottom: 30px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); transition: background-color 0.3s;}\n",
        "        .dark-mode .header-container { background-color: #0056b3; }\n",
        "        .header-container img {max-width: 150px; height: auto; margin-bottom: 10px;}\n",
        "        .form-box, table {background: white; padding: 30px; border-radius: 10px; box-shadow: 0 6px 20px rgba(0,0,0,0.15); max-width: 800px; margin: 30px auto; transition: background-color 0.3s, box-shadow 0.3s;}\n",
        "        .dark-mode .form-box, .dark-mode table { background: #34495e; box-shadow: 0 6px 20px rgba(0,0,0,0.3); }\n",
        "        input, button {width: calc(100% - 24px); padding: 12px; margin: 8px 0; border-radius: 5px; border: 1px solid #ccc; font-size: 16px; transition: background-color 0.3s, color 0.3s, border-color 0.3s;}\n",
        "        .dark-mode input { background: #4a627a; color: #ecf0f1; border-color: #5d7a96; }\n",
        "        button {background: #28a745; color: white; font-size: 18px; cursor: pointer; border: none; transition: background-color 0.3s ease;}\n",
        "        button:hover {background: #218838;}\n",
        "        .dark-mode button { background: #27ae60; }\n",
        "        .dark-mode button:hover { background: #2ecc71; }\n",
        "        .dark-mode .toggle-button { background: #7f8c8d; color: white;}\n",
        "        .dark-mode .toggle-button:hover { background: #95a5a5; }\n",
        "        table {width: 90%; border-collapse: separate; border-spacing: 0; overflow: hidden;}\n",
        "        th, td {padding: 15px; text-align: left; border-bottom: 1px solid #eee; transition: background-color 0.3s, color 0.3s, border-color 0.3s;}\n",
        "        .dark-mode th, .dark-mode td { border-color: #4a627a; }\n",
        "        th {background: #007bff; color: white; font-weight: bold; border-top-left-radius: 8px; border-top-right-radius: 8px; transition: background-color 0.3s;}\n",
        "        .dark-mode th { background: #1a252f; }\n",
        "        tr:last-child td {border-bottom: none;}\n",
        "        .high {background: #f8d7da; color: #721c24;}\n",
        "        .medium {background: #fff3cd; color: #856404;}\n",
        "        .low {background: #d4edda; color: #155724;}\n",
        "        .dark-mode .high { background: #7f3d43; color: #f8d7da; }\n",
        "        .dark-mode .medium { background: #8e7223; color: #fff3cd; }\n",
        "        .dark-mode .low { background: #3b6b44; color: #d4edda; }\n",
        "        .high strong, .high td {color: #721c24;}\n",
        "        .medium strong, .medium td {color: #856404;}\n",
        "        .low strong, .low td {color: #155724;}\n",
        "        .dark-mode .high strong, .dark-mode .high td { color: #f8d7da; }\n",
        "        .dark-mode .medium strong, .dark-mode .medium td { color: #fff3cd; }\n",
        "        .dark-mode .low strong, .dark-mode .low td { color: #d4edda; }\n",
        "    </style>\n",
        "    <script>\n",
        "        function toggleDarkMode() {\n",
        "            document.body.classList.toggle('dark-mode');\n",
        "            // Save preference to localStorage\n",
        "            if (document.body.classList.contains('dark-mode')) {\n",
        "                localStorage.setItem('darkMode', 'enabled');\n",
        "            } else {\n",
        "                localStorage.setItem('darkMode', 'disabled');\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Apply dark mode preference on page load\n",
        "        document.addEventListener('DOMContentLoaded', (event) => {\n",
        "            if (localStorage.getItem('darkMode') === 'enabled') {\n",
        "                document.body.classList.add('dark-mode');\n",
        "            }\n",
        "        });\n",
        "    </script>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"header-container\">\n",
        "        <img src=\"https://www.aun.edu.ng/images/2025/01/24/default-logo-wide---admin.jpg\" alt=\"AUN Logo\">\n",
        "        <h1>AUN Student Attrition Decision Support System</h1>\n",
        "    </div>\n",
        "    <div style=\"text-align:center; margin-bottom: 20px;\">\n",
        "        <button onclick=\"toggleDarkMode()\" class=\"toggle-button\">Toggle Dark Mode</button>\n",
        "    </div>\n",
        "    <div class=\"form-box\">\n",
        "        <h2>Assess New Student Risk</h2>\n",
        "        <form method=\"post\">\n",
        "            <input name=\"sid\" placeholder=\"Student ID\" required>\n",
        "            <input type=\"number\" step=\"0.01\" name=\"gpa\" placeholder=\"GPA (0-4.0)\" required>\n",
        "            <input type=\"number\" step=\"0.01\" name=\"balance\" placeholder=\"Outstanding Balance\" required>\n",
        "            <input type=\"number\" name=\"logins\" placeholder=\"Logins this semester (optional)\">\n",
        "            <input type=\"number\" name=\"missed\" placeholder=\"Classes missed (required)\" required>\n",
        "            <input type=\"number\" name=\"incidents\" placeholder=\"Incidents\" required>\n",
        "            <input type=\"number\" name=\"sessionsAttended\" placeholder=\"Advisory Sessions Attended (optional)\">\n",
        "            <button type=\"submit\">Check Risk Now</button>\n",
        "        </form>\n",
        "    </div>\n",
        "\n",
        "    {% if result %}\n",
        "    <h2 style=\"text-align:center;\">Result for {{ result.id }}</h2>\n",
        "    <table class=\"{{ result.color }}\">\n",
        "        <tr><td><strong>Ontology Risk Level</strong></td><td><strong style=\"color:{{ result.color_text }}\">{{ result.risk }}</strong></td></tr>\n",
        "        <tr><td><strong>ML Risk Score</strong></td><td><strong>{{ result.ml_score }}</strong></td></tr>\n",
        "        <tr><td><strong>Recommended Actions</strong></td><td>{{ result.actions | safe }}</td></tr>\n",
        "    </table>\n",
        "    {% endif %}\n",
        "\n",
        "    <h2 style=\"text-align:center; margin-top:50px;\">Current Students from Ontology</h2>\n",
        "    <table>\n",
        "        <tr><th>Student ID</th><th>Ontology Risk</th><th>ML Risk Score</th><th>Recommended Actions</th></tr>\n",
        "        {% for s in ontology_students %}\n",
        "        <tr class=\"{{ s.color }}\">\n",
        "            <td>{{ s.id }}</td>\n",
        "            <td style=\"color:{{ s.color_text }}; font-weight:bold;\">{{ s.risk }}</td>\n",
        "            <td>{{ s.ml_score }}</td>\n",
        "            <td>{{ s.actions | safe }}</td>\n",
        "        </tr>\n",
        "        {% endfor %}\n",
        "    </table>\n",
        "</body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "@app.route('/', methods=['GET', 'POST'])\n",
        "def home():\n",
        "    result = None\n",
        "    ontology_students = []\n",
        "\n",
        "    try:\n",
        "        print(\"--- Entering home() function ---\")\n",
        "        print(f\"Ontology object: {onto}\") # Check if onto is accessible\n",
        "\n",
        "        # Load your 3 students from ontology\n",
        "        student_ids = ['A00080012', 'A00080013', 'A00080014']\n",
        "        print(f\"Processing student IDs: {student_ids}\")\n",
        "\n",
        "        # Manually provided ML scores\n",
        "        manual_ml_scores = {\n",
        "            'A00080012': 0.9,\n",
        "            'A00080013': 0.4,\n",
        "            'A00080014': 0.01\n",
        "        }\n",
        "\n",
        "        with onto:\n",
        "            for sid in student_ids:\n",
        "                student_iri = f\"http://www.semanticweb.org/muhdm/ontologies/2025/AUN_SRO#{sid}\"\n",
        "                student = onto.search_one(iri=student_iri)\n",
        "                print(f\"Searching for student {sid}. Found: {student is not None}\")\n",
        "                if not student:\n",
        "                    print(f\"Warning: Student {sid} not found in ontology.\")\n",
        "                    continue\n",
        "\n",
        "                # Risk level\n",
        "                risk = \"Unknown\"\n",
        "                if hasattr(student, 'hasRiskLevel') and student.hasRiskLevel:\n",
        "                    risk = student.hasRiskLevel[0] if isinstance(student.hasRiskLevel, list) else student.hasRiskLevel\n",
        "                # Fallback if no inference\n",
        "                if risk == \"Unknown\":\n",
        "                    risk = \"Low\"  # Default to Low if no data\n",
        "\n",
        "                # Recommendations\n",
        "                actions = \"No recommendation inferred, student doing well\"\n",
        "                if hasattr(student, 'recommendedAction') and student.recommendedAction:\n",
        "                    recs = student.recommendedAction\n",
        "                    actions = '<br>'.join(recs if isinstance(recs, list) else [recs])\n",
        "\n",
        "                # ML score: Use manual override for ontology students\n",
        "                ml_score = f\"{manual_ml_scores.get(sid, 0.0):.3f}\"\n",
        "\n",
        "                print(f\"Student {sid} input features for ML: (manual override)\") # Debugging print\n",
        "\n",
        "                color = \"low\"\n",
        "                color_text = \"green\"\n",
        "                if \"High\" in risk:\n",
        "                    color = \"high\"\n",
        "                    color_text = \"red\"\n",
        "                elif \"Medium\" in risk:\n",
        "                    color = \"medium\"\n",
        "                    color_text = \"orange\"\n",
        "\n",
        "                ontology_students.append({\n",
        "                    'id': sid,\n",
        "                    'risk': risk,\n",
        "                    'color': color,\n",
        "                    'color_text': color_text,\n",
        "                    'actions': actions,\n",
        "                    'ml_score': ml_score\n",
        "                })\n",
        "        print(f\"Ontology students loaded: {ontology_students}\")\n",
        "\n",
        "        if request.method == 'POST':\n",
        "            print(\"--- Handling POST request ---\")\n",
        "            try:\n",
        "                sid = request.form['sid']\n",
        "                gpa = float(request.form['gpa'])\n",
        "                balance = float(request.form['balance'])\n",
        "                missed = int(request.form['missed'])\n",
        "                incidents = int(request.form['incidents'])\n",
        "\n",
        "                # Handle optional inputs and create _is_na flags\n",
        "                logins_input = request.form.get('logins', '').strip()\n",
        "                sessionsAttended_input = request.form.get('sessionsAttended', '').strip()\n",
        "\n",
        "                logins_is_na = 1 if not logins_input.isdigit() else 0\n",
        "                sessionsAttended_is_na = 1 if not sessionsAttended_input.isdigit() else 0\n",
        "\n",
        "                ml_logins = int(logins_input) if logins_input.isdigit() else 0\n",
        "                ml_sessionsAttended = int(sessionsAttended_input) if sessionsAttended_input.isdigit() else 0\n",
        "\n",
        "                # Construct the feature vector in the assumed order (8 features)\n",
        "                # Assuming the order is: gpa, balance, logins, missed, incidents, sessionsAttended, logins_is_na, sessionsAttended_is_na\n",
        "                input_data_features = [\n",
        "                    gpa,\n",
        "                    balance,\n",
        "                    ml_logins,\n",
        "                    missed,\n",
        "                    incidents,\n",
        "                    ml_sessionsAttended,\n",
        "                    logins_is_na,\n",
        "                    sessionsAttended_is_na\n",
        "                ]\n",
        "\n",
        "                input_data = np.array([input_data_features])\n",
        "                ml_score = best_model.predict_proba(input_data)[0][1]\n",
        "                ml_score = round(ml_score, 3)\n",
        "\n",
        "                # Initialize actions set to handle duplicates and current_risk_level\n",
        "                actions = set()\n",
        "                current_risk_level_int = 0 # 0=Low, 1=Medium, 2=High\n",
        "\n",
        "                # Apply specific rules to determine max risk level and collect actions\n",
        "                if gpa < 2.0:\n",
        "                    actions.add(\"Urgent SAP review and academic advising\")\n",
        "                    current_risk_level_int = max(current_risk_level_int, 2)\n",
        "\n",
        "                if 2.0 <= gpa < 2.5:\n",
        "                    actions.add(\"Academic monitoring recommended\")\n",
        "                    current_risk_level_int = max(current_risk_level_int, 1)\n",
        "\n",
        "                if balance > 1500:\n",
        "                    actions.add(\"Immediate financial aid consultation\")\n",
        "                    current_risk_level_int = max(current_risk_level_int, 2)\n",
        "\n",
        "                if balance > 500 and balance <= 1500:\n",
        "                    actions.add(\"Review payment plan\") # Changed to match user's rule wording\n",
        "                    current_risk_level_int = max(current_risk_level_int, 1)\n",
        "\n",
        "                if ml_logins < 15:\n",
        "                    actions.add(\"Suggest increasing Canvas/LMS usage to improve engagement\")\n",
        "\n",
        "                if missed > 8:\n",
        "                    actions.add(\"Encourage better class attendance – high number of missed classes detected\")\n",
        "\n",
        "                if incidents > 1: # Corrected threshold\n",
        "                    actions.add(\"Residential life check-in suggested\") # Corrected wording\n",
        "\n",
        "                if ml_score > 0.6:\n",
        "                    actions.add(\"ML model flags high risk – review all factors\")\n",
        "                    current_risk_level_int = max(current_risk_level_int, 2)\n",
        "\n",
        "                if ml_score > 0.5 and ml_score <= 0.6:\n",
        "                    actions.add(\"ML model suggests monitoring\")\n",
        "                    current_risk_level_int = max(current_risk_level_int, 1)\n",
        "\n",
        "                # Determine final risk string based on the highest risk identified\n",
        "                if current_risk_level_int == 2:\n",
        "                    risk = \"High\"\n",
        "                elif current_risk_level_int == 1:\n",
        "                    risk = \"Medium\"\n",
        "                else:\n",
        "                    risk = \"Low\"\n",
        "\n",
        "                # Advisory sessions recommendation (conditional on risk being High or Medium)\n",
        "                # Note: ml_sessionsAttended might be 0 due to imputation if originally missing.\n",
        "                # The rule should consider if it was *provided* or if it was *imputed to 0 and below 2*.\n",
        "                # Using sessionsAttended_is_na to distinguish. If it was provided and < 2.\n",
        "                if not sessionsAttended_is_na and ml_sessionsAttended < 2:\n",
        "                    if risk == \"High\" or risk == \"Medium\":\n",
        "                        actions.add(\"Schedule advising session if risk is high or medium if low not necessary\")\n",
        "                elif sessionsAttended_is_na and (risk == \"High\" or risk == \"Medium\"):\n",
        "                    # If sessionsAttended was not provided, but risk is High/Medium, still recommend scheduling.\n",
        "                    actions.add(\"Schedule advising session if risk is high or medium if low not necessary\")\n",
        "\n",
        "\n",
        "                # Add general recommendations based on the final determined risk level from ONTOLOGY_RECOMMENDATIONS\n",
        "                general_recs = ONTOLOGY_RECOMMENDATIONS[risk].split('<br>')\n",
        "                for rec in general_recs:\n",
        "                    actions.add(rec.strip())\n",
        "\n",
        "                # Convert actions set to list for joining\n",
        "                actions_list = list(actions)\n",
        "\n",
        "                # Default action if no specific actions were triggered and 'Low' risk general rec wasn't already added (should not happen if ONTOLOGY_RECOMMENDATIONS is used)\n",
        "                if not actions_list:\n",
        "                    actions_list = [\"No action needed – student performing well\"]\n",
        "\n",
        "                color = \"high\" if risk == \"High\" else \"medium\" if risk == \"Medium\" else \"low\"\n",
        "                color_text = \"red\" if risk == \"High\" else \"orange\" if risk == \"Medium\" else \"green\"\n",
        "\n",
        "                result = {\n",
        "                    'id': sid,\n",
        "                    'risk': risk,\n",
        "                    'color': color,\n",
        "                    'color_text': color_text,\n",
        "                    'actions': '<br>'.join(actions_list),\n",
        "                    'ml_score': f\"{ml_score:.3f}\"\n",
        "                }\n",
        "                print(f\"POST request result: {result}\")\n",
        "            except Exception as e: # Catch errors specifically during POST request processing\n",
        "                import traceback\n",
        "                print(\"An error occurred during POST request processing:\")\n",
        "                traceback.print_exc()\n",
        "                result = {'id': 'Error', 'risk': 'Invalid input', 'color': 'low', 'color_text': 'black', 'actions': str(e), 'ml_score': 'N/A'}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any error during GET request or initial processing outside POST block\n",
        "        import traceback\n",
        "        print(\"An unhandled error occurred in the home function (GET request or general processing):\")\n",
        "        traceback.print_exc() # This will print the full traceback to Colab's output\n",
        "        result = {'id': 'Global Error', 'risk': 'Server Issue', 'color': 'high', 'color_text': 'red', 'actions': f\"An unexpected error occurred: {e}\", 'ml_score': 'N/A'}\n",
        "\n",
        "    print(\"--- Exiting home() function ---\")\n",
        "    return render_template_string(TEMPLATE, ontology_students=ontology_students, result=result)\n",
        "\n",
        "# Cell 6: Run Flask\n",
        "# Changed port to 5011 to avoid 'Address already in use' error\n",
        "threading.Thread(target=app.run, kwargs={'host':'0.0.0.0','port':5000,'use_reloader': False}).start()\n",
        "\n",
        "from google.colab import output\n",
        "# Changed port to 5011 to avoid 'Address already in use' error\n",
        "output.serve_kernel_port_as_window(5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload AUN_SRO.owl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a3d25413-5d06-42db-a1f5-72979513f46e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a3d25413-5d06-42db-a1f5-72979513f46e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Unexpected exception finding object shape\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/colab/_debugpy_repr.py\", line 54, in get_shape\n",
            "    shape = getattr(obj, 'shape', None)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 318, in __get__\n",
            "    obj = instance._get_current_object()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/werkzeug/local.py\", line 519, in _get_current_object\n",
            "    raise RuntimeError(unbound_message) from None\n",
            "RuntimeError: Working outside of request context.\n",
            "\n",
            "This typically means that you attempted to use functionality that needed\n",
            "an active HTTP request. Consult the documentation on testing for\n",
            "information about how to avoid this problem.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving AUN_SRO.owx to AUN_SRO (3).owx\n",
            "\n",
            "Upload best_rf_model.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a26d80c-42bf-4f8a-b1a2-a94bd2d3e28a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a26d80c-42bf-4f8a-b1a2-a94bd2d3e28a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving retrained_best_model.pkl to retrained_best_model (3).pkl\n",
            "Ontology loaded!\n",
            "ML model loaded!\n",
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(5000, \"/\", \"https://localhost:5000/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}